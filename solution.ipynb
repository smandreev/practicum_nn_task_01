{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автодополнение текста —\n",
    "\n",
    "### Этап 0. Подготовка данных.\n"
   ],
   "id": "b080f8b2f326b9e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T09:14:13.213948Z",
     "start_time": "2026-02-21T09:14:13.019685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import zipfile\n",
    "\n",
    "# Unpack the archive\n",
    "with zipfile.ZipFile('data/raw_data.txt.zip', 'r') as z:\n",
    "    z.extractall('data/')\n",
    "\n",
    "# Read the raw data\n",
    "with open('data/raw_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Original length: {len(text)} chars\")\n",
    "sample_chars = 300\n",
    "print(f\"Sample (first {sample_chars} chars):\")\n",
    "print(text[:sample_chars])"
   ],
   "id": "e0bfd3c3a649ffa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 120184721 chars\n",
      "Sample (first 300 chars):\n",
      "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "44k0ykkfmxv",
   "source": [
    "import emoji\n",
    "\n",
    "# Lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove URLs (http/https/www links)\n",
    "text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "# Remove mentions (@username)\n",
    "text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "# Remove emojis\n",
    "text = emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# Remove all symbols except letters, numbers, and whitespace\n",
    "text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ0-9\\s]', '', text)\n",
    "\n",
    "# Remove duplicate whitespaces (spaces, tabs, etc.) and strip lines\n",
    "text = re.sub(r'[^\\S\\n]+', ' ', text)   # collapse whitespaces to single space\n",
    "text = re.sub(r' *\\n *', '\\n', text)    # clean spaces around newlines\n",
    "text = re.sub(r'\\n{2,}', '\\n', text)    # collapse multiple newlines\n",
    "text = text.strip()\n",
    "\n",
    "print(f\"Cleaned length: {len(text)} chars\")\n",
    "print(f\"\\nCleaned text (first {sample_chars} chars):\")\n",
    "print(text[:sample_chars])"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T09:14:44.116303Z",
     "start_time": "2026-02-21T09:14:13.215779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned length: 101272763 chars\n",
      "\n",
      "Cleaned text (first 300 chars):\n",
      "awww thats a bummer you shoulda got david carr of third day to do it d\n",
      "is upset that he cant update his facebook by texting it and might cry as a result school today also blah\n",
      "i dived many times for the ball managed to save 50 the rest go out of bounds\n",
      "my whole body feels itchy and like its on fire\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "76xjlrkcyku",
   "source": "# Save cleaned data\nwith open('data/clean_data.txt', 'w', encoding='utf-8') as f:\n    f.write(text)\n\nprint(\"Saved to data/clean_data.txt\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T09:14:44.150661Z",
     "start_time": "2026-02-21T09:14:44.130681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/clean_data.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c5patcs7s",
   "source": "### Этап 1. Токенизация и подготовка словаря",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ozfepzoa74s",
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Загрузка очищенных данных\n",
    "with open('data/clean_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Разбиваем текст на токены\n",
    "tokens = text.split()\n",
    "print(f\"Total tokens: {len(tokens):,}\")\n",
    "\n",
    "# Строим словарь из тренинг данных\n",
    "n = len(tokens)\n",
    "train_end = int(0.8 * n)\n",
    "train_tokens = tokens[:train_end]\n",
    "\n",
    "# Считаем частоту слов из выборки для тренинга\n",
    "counter = Counter(train_tokens)\n",
    "VOCAB_SIZE = 20_000  # Размер словаря + <UNK>\n",
    "\n",
    "vocab = {'<UNK>': 0}\n",
    "for word, _ in counter.most_common(VOCAB_SIZE - 1):\n",
    "    vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"Most common words: {counter.most_common(10)}\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open('data/vocab.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved vocabulary to data/vocab.json\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T09:14:45.362310Z",
     "start_time": "2026-02-21T09:14:44.151317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 19,995,368\n",
      "Vocabulary size: 20,000\n",
      "Most common words: [('i', 629331), ('to', 460330), ('the', 411280), ('a', 296646), ('my', 261286), ('and', 237489), ('you', 195161), ('is', 191042), ('it', 184570), ('in', 172956)]\n",
      "Saved vocabulary to data/vocab.json\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "cvg62s4lvbn",
   "source": "### Разбиение данных: Тренировка / Валидация / Тестирование",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0oj1cxzbput",
   "source": [
    "import csv\n",
    "\n",
    "SEQ_LEN = 20  # размер входной последовательности для модели\n",
    "\n",
    "val_end = int(0.9 * n)\n",
    "val_tokens = tokens[train_end:val_end]\n",
    "test_tokens = tokens[val_end:]\n",
    "\n",
    "# сохраняем токены для тренировки\n",
    "with open('data/train_data.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(train_tokens))\n",
    "\n",
    "print(f\"Train: {len(train_tokens):,} tokens -> data/train_data.txt\")\n",
    "\n",
    "# Нарезает список токенов на неперекрывающиеся окна и сохраняет как CSV с двумя колонками: input (входная последовательность) и target (слово, которое нужно предсказать).   Допустим tokens = ['i', 'love', 'my', 'cat', 'and', 'dog', ...] и seq_len = 3. Input =  i love my ; target = cat.\n",
    "def save_as_csv(tokens, filepath, seq_len):\n",
    "    rows = 0\n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['input', 'target'])\n",
    "        for i in range(0, len(tokens) - seq_len, seq_len):\n",
    "            input_seq = ' '.join(tokens[i:i + seq_len])\n",
    "            target = tokens[i + seq_len]\n",
    "            writer.writerow([input_seq, target])\n",
    "            rows += 1\n",
    "    return rows\n",
    "\n",
    "val_rows = save_as_csv(val_tokens, 'data/val_data.csv', SEQ_LEN)\n",
    "test_rows = save_as_csv(test_tokens, 'data/test.csv', SEQ_LEN)\n",
    "\n",
    "print(f\"Val:   {len(val_tokens):,} tokens -> {val_rows:,} samples -> data/val_data.csv\")\n",
    "print(f\"Test:  {len(test_tokens):,} tokens -> {test_rows:,} samples -> data/test.csv\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T09:14:45.853132Z",
     "start_time": "2026-02-21T09:14:45.467658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 15,996,294 tokens -> data/train_data.txt\n",
      "Val:   1,999,537 tokens -> 99,976 samples -> data/val_data.csv\n",
      "Test:  1,999,537 tokens -> 99,976 samples -> data/test.csv\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "d6ijk6vi9y",
   "source": "### Подготовка Dataset и DataLoader",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "kkins76mn5d",
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.next_token_dataset import NextTokenDataset\n",
    "\n",
    "# Load vocabulary\n",
    "with open('data/vocab.json', 'r', encoding='utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Load training tokens\n",
    "with open('data/train_data.txt', 'r', encoding='utf-8') as f:\n",
    "    train_tokens = f.read().split()\n",
    "\n",
    "# Create datasets (val_tokens / test_tokens are already in memory from the split above)\n",
    "train_dataset = NextTokenDataset(train_tokens, vocab, seq_len=SEQ_LEN)\n",
    "val_dataset = NextTokenDataset(val_tokens, vocab, seq_len=SEQ_LEN)\n",
    "\n",
    "# Create DataLoaders\n",
    "use_pin_memory = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, pin_memory=use_pin_memory)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=2, pin_memory=use_pin_memory)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} samples\")\n",
    "print(f\"Val dataset:   {len(val_dataset):,} samples\")\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches:   {len(val_loader):,}\")\n",
    "\n",
    "# Inspect one batch\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes: x={x_batch.shape}, y={y_batch.shape}\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T16:57:08.374016Z",
     "start_time": "2026-02-21T16:57:05.825001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 15,996,274 samples\n",
      "Val dataset:   1,999,517 samples\n",
      "Train batches: 62,486\n",
      "Val batches:   7,811\n",
      "\n",
      "Batch shapes: x=torch.Size([256, 20]), y=torch.Size([256])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "397gkykcib4",
   "source": "## Этап 2. Рекуррентная сеть (LSTM)\n\n### Архитектура модели",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "yjaosgtywla",
   "source": [
    "import torch\n",
    "from src.lstm_model import LSTMLanguageModel\n",
    "\n",
    "VOCAB_SIZE = len(vocab)     # Размер словаря токенов. Определяет количество строк в Embedding-таблице и размер выхода финального Linear-слоя\n",
    "EMBED_DIM  = 128            # Размерность векторов слов. Каждый индекс слова превращается в вектор этой размерности перед подачей в LSTM\n",
    "HIDDEN_DIM = 256            # Размер скрытого слоя LSTM. Че больше, тем больше ёмкость для запоминания контекста, но медленнее и выше риск переобучения\n",
    "NUM_LAYERS = 2              # Количество слоёв LSTM. Для двух слоёв - Слой 1 обрабатывает эмбеддинги, слой 2 — выход слоя 1. Добавляет глубину модели\n",
    "DROPOUT    = 0.3            # Вероятность обнуления выходов между слоями LSTM (0.3 = 30%). Применяется только между слоями, поэтому игнорируется при num_layers == 1. Регуляризация для снижения переобучения\n",
    "\n",
    "\n",
    "# MPS - для запуска на M4Max\n",
    "# CUDA - для запуска на NVidia Jetson Orin Nano или на видео-карте\n",
    "# CPU - по дефолту\n",
    "device = torch.device('mps' if torch.backends.mps.is_available()\n",
    "                       else 'cuda' if torch.cuda.is_available()\n",
    "                       else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(model)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T10:06:17.368853Z",
     "start_time": "2026-02-21T10:06:17.307133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Model parameters: 8,621,600\n",
      "LSTMLanguageModel(\n",
      "  (embedding): Embedding(20000, 128)\n",
      "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=20000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "l2v6jsle3uj",
   "source": "### Проверка forward pass",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7hep2o942hk",
   "source": "# Test forward pass with a single batch\nx_test = x_batch.to(device)   # (256, 20)\nlogits, hidden = model(x_test)\n\nprint(f\"Input shape:   {x_test.shape}\")\nprint(f\"Logits shape:  {logits.shape}\")    # expected: (256, 20000)\nprint(f\"Hidden h shape:{hidden[0].shape}\") # expected: (num_layers, 256, hidden_dim)\nprint(f\"Hidden c shape:{hidden[1].shape}\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T10:07:35.169391Z",
     "start_time": "2026-02-21T10:07:34.718895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:   torch.Size([256, 20])\n",
      "Logits shape:  torch.Size([256, 20000])\n",
      "Hidden h shape:torch.Size([2, 256, 256])\n",
      "Hidden c shape:torch.Size([2, 256, 256])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "1mmq5tv2msq",
   "source": "### Проверка генерации текста (до обучения — случайные веса)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m1rn7n59cp8",
   "source": [
    "# Build reverse mapping: index -> word\n",
    "idx2word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "#prompt = ['i', 'love', 'machine', 'learning']\n",
    "prompt = ['is','upset', 'that', 'he', 'cant', 'update', 'his', 'facebook', 'by', 'texting', 'it', 'and', 'might']\n",
    "generated = model.generate(\n",
    "    prompt=prompt,\n",
    "    vocab=vocab,\n",
    "    idx2word=idx2word,\n",
    "    max_new_tokens=20,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Prompt:    \", ' '.join(prompt))\n",
    "print(\"Generated: \", ' '.join(generated))\n",
    "print(\"Full text: \", ' '.join(prompt + generated))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T17:11:00.684198Z",
     "start_time": "2026-02-21T17:11:00.618772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:     is upset that he cant update his facebook by texting it and might\n",
      "Generated:  12pm oi boobie eye hopes zane ignorance bullies tue workstation convos mabye congratulate never katie cravin fork willing schools siento\n",
      "Full text:  is upset that he cant update his facebook by texting it and might 12pm oi boobie eye hopes zane ignorance bullies tue workstation convos mabye congratulate never katie cravin fork willing schools siento\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
