# practicum_nn_task_01

## Яндекс Практикум. Проект "Нейросеть для автодополнения текстов".
## Андреев С.М.

### Рекомендованная структура проекта
/
├── data/                            # Датасеты

│   ├── raw_dataset.csv              # "сырой" скачанный датасет

│   └── dataset_processed.csv        # "очищенный" датасет

│   ├── train.csv                    # тренировочная выборка

│   ├── val.csv                      # валидационная выборка

│   └── test.csv                     # тестовая выборка

│

├── src/                             # Весь код проекта

│   ├── data_utils.py                # Обработка датасета

|   ├── next_token_dataset.py        # код с torch Dataset'ом 

│   ├── lstm_model.py                # код lstm модели

|   ├── eval_lstm.py                 # замер метрик lstm модели

|   ├── lstm_train.py                # код обучения модели

|   ├── eval_transformer_pipeline.py # код с запуском и замером качества трансформера

│

├── configs/                         # yaml-конфиги с настройками проекта

│

├── models/                          # веса обученных моделей

|

├── solution.ipynb                   # ноутбук с решением

└── requirements.txt                 # зависимости проекта 



### Этап 0. Подготовка окружения
- Подготовьте окружение
- Создайте git-репозиторий

### Этап 1. Сбор и подготовка данных
Скачайте проект
- Распаковка raw_data.txt.zip
- Очистка raw_data: преобразование в нижний регистр, удаление имен пользователей и ссылок, удаление символов, не являющихся цифрами и буквами, удаление лишних пробелов и символов новой строки
- Токенизация набора данных (на уровне слов, 20000 самых популярных слов + <UNK>). Словарь сохранен в data/vocab.json.
- Разделение набора данных на обучающую выборку (80%), валидацию (10%) и тестовую выборку (10%). Сохранено в train_data.txt / val_data.csv / test.csv.
- Создать torch.Dataset (src/next_token_dataset.py) и DataLoaders (seq_len=20, batch_size=256).

### Этап 2. Реализация рекуррентной сети
- Напишите код модели на основе LSTM. В методе forward модель должна принимать на вход последовательность токенов и предсказывать следующий токен.
- Дополнительно для модели реализуйте метод генерации нескольких токенов.

### Этап 3. Тренировка модели
- Напишите код замера и вывода метрики ROUGE. В коде модель должна проходиться по DataLoader'у и генерировать автодополнения, которые затем будут сравниваться с таргетом. 
- Для простоты реализуйте сценарий, где в качестве входа модель получает 3/4 исходного текста и старается дополнить оставшиеся 1/4.
- Напишите код тренировки модели, во время тренировки выводите значения функции потерь и метрики ROUGE.
- Обучите модель, подобрав оптимальные параметры. Во время тренировки или после неё выведите некоторые примеры автодополнений, которые выучила модель. Ориентировочное время тренировки: несколько минут на одну эпоху при размере батча около 256 и размерности скрытого слоя LSTM около 128. Так как тексты ограничены по длине, а памяти на видеокарте хватает с запасом, то можно ставить размер батча и размерности модели и побольше.

### Этап 4. Использование предобученного трансформера
- Воспользуйтесь моделью трансформера distilgpt2 из Transformers и дополните тексты из датасета.
- Напишите код замера и вывода метрики ROUGE, но уже с использованием трансформера. Здесь, как и с LSTM, предсказывайте последнюю четверть текста.
- Подберите параметры генерации, замерьте качество модели на валидационной выборке, выведите примеры предсказаний.

### Этап 5. Формулирование выводов
- Сравните примеры предсказаний двух моделей, а также получившиеся метрики.
- Сделайте выводы о том, какую модель лучше использовать и почему.

Статистика по работе всего блокнота solution.ipynb:
1. MacBook c M4Max + 128GB RAM, запуск на MPS ~ 4h 30m 
2. Yandex VM ~ ToDo
3. NVidia Orin Jetson Nano - ToDo
4. DeskTop Ubuntu + RTX 3090 - ToDo
5. LapTop Ubuntu + RTX 4080 mobile - ToDo